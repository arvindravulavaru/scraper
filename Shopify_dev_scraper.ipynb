{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19_IMLB5N0NlJ6_TUTkub5vfkssVms6g3",
      "authorship_tag": "ABX9TyP+BcExyGsb6EYBVXhK2P1B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvindravulavaru/scraper/blob/main/Shopify_dev_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create folder structue in your Google Drive before running the code!"
      ],
      "metadata": {
        "id": "2r9AQUd9SIzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install html2text\n"
      ],
      "metadata": {
        "id": "Ik_cVF9T0i2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ee3f871-76fc-4edb-9431-747d46b8f678"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting html2text\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: html2text\n",
            "Successfully installed html2text-2020.1.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import html2text\n",
        "import datetime\n",
        "import pytz\n",
        "import os\n",
        "import tarfile\n",
        "import json\n",
        "from urllib.parse import urlparse, parse_qs, urljoin\n",
        "\n",
        "now = datetime.datetime.now(pytz.timezone('US/Eastern')).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def remove_tags(soup, tag_names):\n",
        "    \"\"\"\n",
        "    This function removes specified tags from HTML.\n",
        "\n",
        "    Parameters:\n",
        "    soup (bs4.BeautifulSoup) : BeautifulSoup object\n",
        "    tag_names (list) : list of strings\n",
        "\n",
        "    Returns:\n",
        "    str : cleaned HTML\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove specified tags from HTML\n",
        "    for tag_name in tag_names:\n",
        "        for tag in soup.find_all(tag_name):\n",
        "            tag.decompose()\n",
        "\n",
        "    # Return cleaned HTML\n",
        "    return str(soup)\n",
        "\n",
        "\n",
        "# Function to scrape links recursively\n",
        "def scrape_links(url, folder_path, max_files, scraped_links, tag_names, paths):\n",
        "\n",
        "    # Check if we have reached the maximum number of files\n",
        "    if len(scraped_links) >= max_files:\n",
        "        return scraped_links\n",
        "    \n",
        "    # Write content to files\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    # Send GET request to URL if it is from the shopify domain\n",
        "    if \"shopify.dev\" not in urlparse(url).netloc:\n",
        "        return scraped_links\n",
        "    \n",
        "    print(f'Processing URL: [{len(scraped_links) + 1}] {url}')\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    # Find the element with class name 'article--docs'\n",
        "    article_html = soup.find(class_='article--docs')    \n",
        "\n",
        "    if article_html is not None and len(str(article_html.text)) > 1:\n",
        "      main_article_soup = BeautifulSoup(str(article_html), 'html.parser')\n",
        "\n",
        "      # Remove specified tags from HTML\n",
        "      page_html = remove_tags(main_article_soup, tag_names)\n",
        "\n",
        "      # Convert HTML to Markdown\n",
        "      converter = html2text.HTML2Text()\n",
        "      converter.body_width = 0\n",
        "      page_markdown = converter.handle(page_html)\n",
        "\n",
        "      # Extract text content from HTML\n",
        "      page_text = soup.get_text()\n",
        "      \n",
        "      file_name = parsed_url.path[1:].replace('/', '-')\n",
        "      file_name = file_name.replace('https-', '').replace('www-', '').replace(':', '').replace('.', '-')\n",
        "\n",
        "      # Create directory if it doesn't exist\n",
        "      file_path = os.path.join(folder_path, file_name)\n",
        "      if not os.path.exists(file_path):\n",
        "          os.makedirs(file_path)\n",
        "\n",
        "      # Write HTML content to file\n",
        "      with open(f'{file_path}/index.html', 'w') as f:\n",
        "          f.write(f\"<!-- Generated on: {now} EST -->\\n\")\n",
        "          f.write(f\"<!-- Scraped URL: {url} -->\\n\")\n",
        "          f.write(page_html)\n",
        "\n",
        "      # Write Markdown content to file\n",
        "      with open(f'{file_path}/index.md', 'w') as f:\n",
        "          f.write(f\"<!-- Generated on: {now} EST -->\\n\")\n",
        "          f.write(f\"<!-- Scraped URL: {url} -->\\n\")\n",
        "          f.write(page_markdown)\n",
        "\n",
        "      # Write text content to file\n",
        "      with open(f'{file_path}/index.txt', 'w') as f:\n",
        "          f.write(f\"<!-- Generated on: {now} EST -->\\n\")\n",
        "          f.write(f\"<!-- Scraped URL: {url} -->\\n\")\n",
        "          f.write(page_text)\n",
        "      \n",
        "      paths.append({\"link\": url, \"path\": file_name, \"title\": soup.title.string})\n",
        "    else:\n",
        "      print(\"No matching element found with class 'article--docs'.\")\n",
        " \n",
        "    # Add link to list of scraped links\n",
        "    scraped_links.append(url)\n",
        "\n",
        "    # Recursively scrape links on the page\n",
        "    for link in soup.find_all('a'):\n",
        "        href = link.get('href')\n",
        "        if href is not None:\n",
        "            href = urljoin(url, href) # convert to absolute URL\n",
        "            \n",
        "            if href.startswith('http') and '#' not in href and href not in scraped_links:\n",
        "                scrape_links(href, folder_path, max_files, scraped_links, tag_names, paths)\n",
        "                \n",
        "                # Check if we have reached the maximum number of files\n",
        "                if len(scraped_links) >= max_files:\n",
        "                    return scraped_links\n",
        "\n",
        "    return scraped_links\n",
        "\n",
        "\n",
        "# Scrape Shopify Developer Documentation website\n",
        "url = \"https://shopify.dev/docs/apps\"\n",
        "# make sure path exists on drive before running the code!\n",
        "folder_path = '/content/drive/MyDrive/Experiments/Data/shopify-dev/'\n",
        "# parent_dir = os.path.abspath(os.path.join(folder_path, os.pardir))\n",
        "parent_dir = '/content/drive/MyDrive/Experiments/Data/'\n",
        "max_files = 10\n",
        "tag_names = ['script', 'style', 'link']\n",
        "tar_filename = 'shopify-dev-files.'+ now + '.tar.gz'\n",
        "meta_data_filename = 'meta-data.'+ now + '.json'\n",
        "paths = []\n",
        "# Scrape links recursively\n",
        "scraped_links = scrape_links(url, folder_path, max_files, [], tag_names, paths)\n",
        "\n",
        "# create a meta data about all files in the folder_path directory\n",
        "with open(os.path.join(os.path.dirname(parent_dir), f'{meta_data_filename}'), \"w\") as file:\n",
        "        json.dump(paths, file, indent=4)\n",
        "\n",
        "# create a tarball of all files in the folder_path directory\n",
        "with tarfile.open(os.path.join(os.path.dirname(parent_dir), f'{tar_filename}'), 'w:gz') as tar:\n",
        "    for file in os.listdir(folder_path):\n",
        "        tar.add(os.path.join(folder_path, file), arcname=file)\n",
        "\n",
        "print(f'Processed {len(paths)} links!')\n",
        "print(f'Successfully created: {tar_filename} & {meta_data_filename}')"
      ],
      "metadata": {
        "id": "RktIM8ZUzEkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177ea144-5ff5-48cc-aab3-8efe6d9e603d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing URL: [1] https://shopify.dev/docs/apps\n",
            "No matching element found with class 'article--docs'.\n",
            "Processing URL: [2] https://shopify.dev/docs\n",
            "No matching element found with class 'article--docs'.\n",
            "Processing URL: [3] https://shopify.dev/apps\n",
            "No matching element found with class 'article--docs'.\n",
            "Processing URL: [4] https://shopify.dev/themes\n",
            "No matching element found with class 'article--docs'.\n",
            "Processing URL: [5] https://shopify.dev/custom-storefronts\n",
            "No matching element found with class 'article--docs'.\n",
            "Processing URL: [6] https://shopify.dev/marketplaces\n",
            "No matching element found with class 'article--docs'.\n",
            "Processing URL: [7] https://shopify.dev/api/admin-graphql\n",
            "No matching element found with class 'article--docs'.\n",
            "Processing URL: [8] https://shopify.dev/api/admin-rest\n",
            "No matching element found with class 'article--docs'.\n",
            "Processing URL: [9] https://shopify.dev/api/liquid\n",
            "No matching element found with class 'article--docs'.\n",
            "Processing URL: [10] https://shopify.dev/api/ajax\n",
            "Processed 1 links!\n",
            "Successfully created: shopify-dev-files.2023-03-19 12:52:57.tar.gz & meta-data.2023-03-19 12:52:57.json\n"
          ]
        }
      ]
    }
  ]
}
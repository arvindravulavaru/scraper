{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19_IMLB5N0NlJ6_TUTkub5vfkssVms6g3",
      "authorship_tag": "ABX9TyNdPWQf2664m6h8Z5fOm1Li",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvindravulavaru/scraper/blob/main/Shopify_dev_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install html2text\n"
      ],
      "metadata": {
        "id": "Ik_cVF9T0i2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import html2text\n",
        "import datetime\n",
        "import pytz\n",
        "import os\n",
        "import tarfile\n",
        "from urllib.parse import urlparse, parse_qs, urljoin\n",
        "\n",
        "# Function to remove script, style, and link tags\n",
        "def remove_tags(soup, tag_names):\n",
        "    \n",
        "    # Remove specified tags from HTML\n",
        "    for tag_name in tag_names:\n",
        "        for tag in soup.find_all(tag_name):\n",
        "            tag.decompose()\n",
        "\n",
        "    # Return cleaned HTML\n",
        "    return str(soup)\n",
        "\n",
        "\n",
        "# Function to scrape links recursively\n",
        "def scrape_links(url, folder_path, max_files, scraped_links, tag_names):\n",
        "\n",
        "    # Check if we have reached the maximum number of files\n",
        "    if len(scraped_links) >= max_files:\n",
        "        return scraped_links\n",
        "\n",
        "    # Write content to files\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    # Send GET request to URL if it is from the shopify domain\n",
        "    if \"shopify.dev\" not in urlparse(url).netloc:\n",
        "        return scraped_links\n",
        "    \n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Remove specified tags from HTML\n",
        "    page_html = remove_tags(soup, tag_names)\n",
        "\n",
        "    # Convert HTML to Markdown\n",
        "    converter = html2text.HTML2Text()\n",
        "    converter.body_width = 0\n",
        "    page_markdown = converter.handle(page_html)\n",
        "\n",
        "    # Extract text content from HTML\n",
        "    page_text = soup.get_text()\n",
        "    \n",
        "    # query_string = parse_qs(parsed_url.query, keep_blank_values=True)\n",
        "    # query_string_formatted = \"_\".join([f\"{k}={v[0]}\" for k, v in query_string.items()]) if query_string else \"\"\n",
        "\n",
        "    file_name = parsed_url.path[1:].replace('/', '-')\n",
        "    file_name = file_name.replace('https-', '').replace('www-', '').replace(':', '').replace('.', '-')\n",
        "    \n",
        "    now = datetime.datetime.now(pytz.timezone('US/Eastern')).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "    # Write HTML content to file\n",
        "    with open(f'{file_path}/index.html', 'w') as f:\n",
        "        f.write(f\"<!-- Generated on: {now} EST -->\\n\")\n",
        "        f.write(f\"<!-- Scraped URL: {url} -->\\n\")\n",
        "        f.write(page_html)\n",
        "\n",
        "    # Write Markdown content to file\n",
        "    with open(f'{file_path}/index.md', 'w') as f:\n",
        "        f.write(f\"<!-- Generated on: {now} EST -->\\n\")\n",
        "        f.write(f\"<!-- Scraped URL: {url} -->\\n\")\n",
        "        f.write(page_markdown)\n",
        "\n",
        "    # Write text content to file\n",
        "    with open(f'{file_path}/index.txt', 'w') as f:\n",
        "        f.write(f\"<!-- Generated on: {now} EST -->\\n\")\n",
        "        f.write(f\"<!-- Scraped URL: {url} -->\\n\")\n",
        "        f.write(page_text)\n",
        "\n",
        "    # Add link to list of scraped links\n",
        "    scraped_links.append(url)\n",
        "\n",
        "    # Recursively scrape links on the page\n",
        "    for link in soup.find_all('a'):\n",
        "        href = link.get('href')\n",
        "        if href is not None:\n",
        "            href = urljoin(url, href) # convert to absolute URL\n",
        "            \n",
        "            if href.startswith('http') and '#' not in href and href not in scraped_links:\n",
        "                scrape_links(href, file_path, max_files, scraped_links, tag_names)\n",
        "                \n",
        "                # Check if we have reached the maximum number of files\n",
        "                if len(scraped_links) >= max_files:\n",
        "                    return scraped_links\n",
        "\n",
        "    return scraped_links\n",
        "\n",
        "\n",
        "# Scrape Shopify Developer Documentation website\n",
        "url = \"https://shopify.dev/docs/apps\"\n",
        "folder_path = '/content/drive/MyDrive/Experiments/Data/shopify-dev/'\n",
        "parent_dir = os.path.abspath(os.path.join(folder_path, os.pardir))\n",
        "max_files = 10\n",
        "tag_names = ['script', 'style', 'link']\n",
        "tar_filename = 'shopify-dev-files.tar.gz'\n",
        "\n",
        "# Scrape links recursively\n",
        "scraped_links = scrape_links(url, folder_path, max_files, [], tag_names)\n",
        "\n",
        "# Print success message\n",
        "print(f'Successfully wrote {len(scraped_links)} files to Google Drive')\n",
        "# # Print site hierarchy\n",
        "# print('Site hierarchy:')\n",
        "# for root, dirs, files in os.walk(folder_path):\n",
        "#     level = root.replace(folder_path, '').count(os.sep)\n",
        "#     indent = ' ' * 2 * (level)\n",
        "#     print(f'{indent}{os.path.basename(root)}/')\n",
        "#     subindent = ' ' * 2 * (level + 1)\n",
        "#     for f in files:\n",
        "#         print(f'{subindent}{f}')\n",
        "\n",
        "# create a tarball of all files in the folder_path directory\n",
        "with tarfile.open(f\"{parent_dir}/{tar_filename}.tar.gz\", 'w:gz') as tar:\n",
        "    for file in os.listdir(folder_path):\n",
        "        tar.add(os.path.join(folder_path, file), arcname=file)\n",
        "\n",
        "print(f'Successfully created tarball: {tar_filename}')"
      ],
      "metadata": {
        "id": "RktIM8ZUzEkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3034f5b-2e69-410e-f88f-625df8322a92"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully wrote 10 files to Google Drive\n",
            "Successfully created tarball: shopify-dev-files.tar.gz\n"
          ]
        }
      ]
    }
  ]
}
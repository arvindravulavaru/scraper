{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19_IMLB5N0NlJ6_TUTkub5vfkssVms6g3",
      "authorship_tag": "ABX9TyMqSxADpKgt5eiiTp4bfcZl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvindravulavaru/scraper/blob/main/Shopify_dev_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install html2text\n"
      ],
      "metadata": {
        "id": "Ik_cVF9T0i2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import html2text\n",
        "import datetime\n",
        "import pytz\n",
        "import os\n",
        "import tarfile\n",
        "from urllib.parse import urlparse, parse_qs, urljoin\n",
        "\n",
        "now = datetime.datetime.now(pytz.timezone('US/Eastern')).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "def remove_tags(soup, tag_names):\n",
        "    \"\"\"\n",
        "    This function removes specified tags from HTML.\n",
        "\n",
        "    Parameters:\n",
        "    soup (bs4.BeautifulSoup) : BeautifulSoup object\n",
        "    tag_names (list) : list of strings\n",
        "\n",
        "    Returns:\n",
        "    str : cleaned HTML\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove specified tags from HTML\n",
        "    for tag_name in tag_names:\n",
        "        for tag in soup.find_all(tag_name):\n",
        "            tag.decompose()\n",
        "\n",
        "    # Return cleaned HTML\n",
        "    return str(soup)\n",
        "\n",
        "\n",
        "# Function to scrape links recursively\n",
        "def scrape_links(url, folder_path, max_files, scraped_links, tag_names):\n",
        "    \"\"\"\n",
        "    This function scrapes links recursively, writes content to files, and returns scraped links.\n",
        "\n",
        "    Parameters:\n",
        "    url (str) : string representing URL to scrape\n",
        "    folder_path (str) : string representing folder path where scraped files are stored\n",
        "    max_files (int) : maximum number of files to scrape\n",
        "    scraped_links (list) : list of strings representing URLs already scraped\n",
        "    tag_names (list) : list of strings representing tags to remove from HTML\n",
        "\n",
        "    Returns:\n",
        "    list : list of strings representing scraped URLs\n",
        "    \"\"\"\n",
        "    \n",
        "    # Check if we have reached the maximum number of files\n",
        "    if len(scraped_links) >= max_files:\n",
        "        return scraped_links\n",
        "\n",
        "    # Write content to files\n",
        "    parsed_url = urlparse(url)\n",
        "\n",
        "    # Send GET request to URL if it is from the shopify domain\n",
        "    if \"shopify.dev\" not in urlparse(url).netloc:\n",
        "        return scraped_links\n",
        "    \n",
        "    print(f'Processing URL: [{len(scraped_links) + 1}] {url}')\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Parse HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Remove specified tags from HTML\n",
        "    page_html = remove_tags(soup, tag_names)\n",
        "\n",
        "    # Convert HTML to Markdown\n",
        "    converter = html2text.HTML2Text()\n",
        "    converter.body_width = 0\n",
        "    page_markdown = converter.handle(page_html)\n",
        "\n",
        "    # Extract text content from HTML\n",
        "    page_text = soup.get_text()\n",
        "    \n",
        "    # query_string = parse_qs(parsed_url.query, keep_blank_values=True)\n",
        "    # query_string_formatted = \"_\".join([f\"{k}={v[0]}\" for k, v in query_string.items()]) if query_string else \"\"\n",
        "\n",
        "    file_name = parsed_url.path[1:].replace('/', '-')\n",
        "    file_name = file_name.replace('https-', '').replace('www-', '').replace(':', '').replace('.', '-')\n",
        "\n",
        "    # Create directory if it doesn't exist\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        os.makedirs(file_path)\n",
        "\n",
        "    # Write HTML content to file\n",
        "    with open(f'{file_path}/index.html', 'w') as f:\n",
        "        f.write(f\"<!-- Generated on: {now} EST -->\\n\")\n",
        "        f.write(f\"<!-- Scraped URL: {url} -->\\n\")\n",
        "        f.write(page_html)\n",
        "\n",
        "    # Write Markdown content to file\n",
        "    with open(f'{file_path}/index.md', 'w') as f:\n",
        "        f.write(f\"<!-- Generated on: {now} EST -->\\n\")\n",
        "        f.write(f\"<!-- Scraped URL: {url} -->\\n\")\n",
        "        f.write(page_markdown)\n",
        "\n",
        "    # Write text content to file\n",
        "    with open(f'{file_path}/index.txt', 'w') as f:\n",
        "        f.write(f\"<!-- Generated on: {now} EST -->\\n\")\n",
        "        f.write(f\"<!-- Scraped URL: {url} -->\\n\")\n",
        "        f.write(page_text)\n",
        "\n",
        "    # Add link to list of scraped links\n",
        "    scraped_links.append(url)\n",
        "\n",
        "    # Recursively scrape links on the page\n",
        "    for link in soup.find_all('a'):\n",
        "        href = link.get('href')\n",
        "        if href is not None:\n",
        "            href = urljoin(url, href) # convert to absolute URL\n",
        "            \n",
        "            if href.startswith('http') and '#' not in href and href not in scraped_links:\n",
        "                scrape_links(href, folder_path, max_files, scraped_links, tag_names)\n",
        "                \n",
        "                # Check if we have reached the maximum number of files\n",
        "                if len(scraped_links) >= max_files:\n",
        "                    return scraped_links\n",
        "\n",
        "    return scraped_links\n",
        "\n",
        "\n",
        "# Scrape Shopify Developer Documentation website\n",
        "url = \"https://shopify.dev/docs/apps\"\n",
        "# make sure path exists on drive before running the code!\n",
        "folder_path = '/content/drive/MyDrive/Experiments/Data/shopify-dev/'\n",
        "# parent_dir = os.path.abspath(os.path.join(folder_path, os.pardir))\n",
        "parent_dir = '/content/drive/MyDrive/Experiments/Data/'\n",
        "max_files = 1\n",
        "tag_names = ['script', 'style', 'link']\n",
        "tar_filename = 'shopify-dev-files.'+ now + '.tar.gz'\n",
        "sitemap_filename = 'site_hierarchy.txt'\n",
        "\n",
        "# Scrape links recursively\n",
        "scraped_links = scrape_links(url, folder_path, max_files, [], tag_names)\n",
        "\n",
        "# Print success message\n",
        "print(f'Successfully wrote {len(scraped_links)} files to Google Drive')\n",
        "# Write site hierarchy to file\n",
        "# with open(os.path.join(os.path.dirname(folder_path), f'{sitemap_filename}'), 'w') as f:\n",
        "#     f.write(f'Site hierarchy:\\n<!-- Generated on: {now} EST -->\\n\\n')\n",
        "#     for root, dirs, files in os.walk(folder_path):\n",
        "#         level = root.replace(folder_path, '').count(os.sep)\n",
        "#         indent = ' ' * 2 * (level)\n",
        "#         f.write(f'{indent}{os.path.basename(root)}/\\n')\n",
        "#         subindent = ' ' * 2 * (level + 1)\n",
        "#         for fx in files:\n",
        "#             f.write(f'{subindent}{fx}\\n')\n",
        "\n",
        "print(os.path.join(os.path.dirname(parent_dir), f'{tar_filename}'))\n",
        "\n",
        "# create a tarball of all files in the folder_path directory\n",
        "with tarfile.open(os.path.join(os.path.dirname(parent_dir), f'{tar_filename}'), 'w:gz') as tar:\n",
        "    for file in os.listdir(folder_path):\n",
        "        tar.add(os.path.join(folder_path, file), arcname=file)\n",
        "\n",
        "print(f'Successfully created tarball: {tar_filename}')"
      ],
      "metadata": {
        "id": "RktIM8ZUzEkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095ea459-9ee0-4bb9-e313-f4a8d6053a85"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing URL: [1] https://shopify.dev/docs/apps\n",
            "Successfully wrote 1 files to Google Drive\n",
            "/content/drive/MyDrive/Experiments/Data/shopify-dev-files.2023-03-18 11:11:54.tar.gz\n",
            "Successfully created tarball: shopify-dev-files.2023-03-18 11:11:54.tar.gz\n"
          ]
        }
      ]
    }
  ]
}